# offline_chat_bot/config.yaml

# LLM Service Wrapper (LSW) Configuration
lsw:
  ollama_host: "http://localhost:11434" # Usually fine as is.
  default_chat_model: "gemma3:12b-it-fp16" # IS THIS YOUR BEST/PREFERRED GENERAL MODEL?
                                         # Ensure it's pulled in Ollama.
  default_embedding_model_ollama: "nomic-embed-text" # Good default, ensure pulled.
  default_embedding_model_st: "all-MiniLM-L6-v2"    # Good general ST model.

# Tokenizer Utility Configuration
tokenizer:
  hf_tokenizer_map: # Keys should be lowercase Ollama model tags or family names
    "gemma3:12b-it-fp16": "google/gemma-3-12b-it"  # GOOD - specific for your main model
    "gemma3:4b-it-fp16": "google/gemma-3-4b-it"  # GOOD - specific for your main model
    "gemma3:1b-it-fp16": "google/gemma-3-4b-it"  # GOOD - 4b tokenizer is fine for 1b Gemma3
    "gemma:2b": "google/gemma-2b"                # For Gemma 2 if used
    "default_gemma": "google/gemma-3-4b-it"      # GOOD - Fallback for any "gemma" model
    "default_llama": "meta-llama/Llama-2-7b-hf"  # Placeholder - update if you use Llama 2/3
    "default_mistral": "mistralai/Mistral-7B-v0.1" # Placeholder - update if you use Mistral
  default_hf_tokenizer: "google/gemma-3-4b-it"     # Overall fallback if no map/family match. GOOD.

# Memory Management Unit (MMU) Configuration
mmu:
  stm_max_turns: 10 # Reasonable default.
  
  mtm_use_tinydb: true # Good for persistence.
  mtm_db_path: "data/mtm_store.json" 

  ltm_sqlite_db_path: "data/ltm_database.db" 
  ltm_chroma_persist_directory: "data/ltm_vector_store" 
  
  ltm_vector_store_embedding_source: "ollama"  # "st" (SentenceTransformer via LSW) or "ollama" (Ollama model via LSW)
                                           # "st" is good for local, consistent embeddings if ST model is good.
  # ltm_vector_store_embedding_model_name: "all-MiniLM-L6-v2" # Optional: if source is 'st' and you want to override LSW's default_st_model for LTM VS.
                                                              # Or if source is 'ollama' and you want to override LSW's default_ollama_model for LTM VS.
                                                              # If commented out, LSWEmbeddingFunctionForMMU will use LSW's configured default for the chosen source.

# Conversation Orchestrator (CO) Configuration
orchestrator:
  # default_llm_model: "gemma3:4b-it-fp16" # If commented out, CO uses lsw.default_chat_model. Explicit is fine too.
  target_max_prompt_tokens: 7000 # Good for an 8k effective context window.
  default_top_k_vector_search: 3 # Reasonable.
  min_tokens_for_user_statement_to_vector_store: 10 # Good threshold.
  max_tokens_per_ltm_vector_chunk: 500 # Good threshold to trigger summarization.
  target_summary_tokens_for_ltm_chunk: 150 # Good target for summaries.

  # --- New STM Management Settings ---
  manage_stm_within_prompt: true              # Enable/disable STM management in prompt building
  stm_condensation_strategy: "summarize"       # "truncate" or "summarize" (implement truncate first)
  target_stm_tokens_budget_ratio: 0.5         # Ratio of remaining token budget for STM (e.g., 0.5 means STM can use 50% of what's left after sys prompt & LTM)
  stm_summary_target_tokens: 200              # Target tokens for a summarized STM block (if strategy is "summarize")
  # --- End New STM Management Settings ---

  fact_extraction:
    min_meaningful_length_for_filter: 3
    common_fillers_for_filter: [
      "is", "are", "was", "were", "am", "be", "the", "a", "an", 
      "my", "me", "i", "it", "this", "that", "and", "or", "for", "to",
      "he", "she", "they", "we", "his", "her", "its", "their", "our",
      "what", "where", "when", "who", "why", "how", "do", "can", 
      "will", "may", "should", "could", "would", "not"
    ]
    question_indicators_for_filter: [
      "what", "where", "when", "who", "why", "how", 
      "do you", "can you", "is there", "are there", "is your"
    ]
  # --- ADD THIS NEW SECTION (ensure indentation matches your orchestrator section) ---
  react_system_prompt_template: |
    You are a highly capable and helpful AI assistant. Your primary goal is to provide accurate, relevant, and coherent responses to the user. You have access to your short-term conversation memory and relevant information retrieved from your long-term knowledge base.

    You also have access to a set of tools to help you answer user requests or perform specific actions.

    **Tool Usage Instructions:**

    1.  **Decision to Use a Tool:** Based on the user's query and the current context (conversation history, retrieved knowledge), decide if using a tool would be beneficial.
    2.  **Output Format for Tool Call:** If you decide to use a tool, your response MUST start *exactly* with the line `TOOL_CALL:` followed by a JSON object enclosed in a markdown code block (```json ... ```) on the subsequent lines. This JSON object must contain:
        *   A `tool_name` key: The name of the tool you want to use.
        *   An `arguments` key: An object containing the arguments for that tool, matching its specified schema.
        Example of a tool call:
        TOOL_CALL:
        ```json
        {{
            "tool_name": "knowledge_search",
            "arguments": {{
                "query_text": "What is my project's deadline?"
            }}
        }}
        ```
    3.  **No Tool Needed (Direct Answer):** If you do not need to use a tool, simply provide your answer or response directly as plain text. Do NOT use the `TOOL_CALL:` prefix.

    **Available Tools:**
    {available_tools_description_string}

    **Using Tool Results (Observations):**

    If you call a tool, I will execute it and provide its output back to you in the next turn, prefixed with `OBSERVATION:`. You should then use this observation, along with the rest of the context, to formulate your final answer or decide on the next step (which could be another tool call or a direct answer).

    **General Guidelines:**

    *   Always prioritize information from the `CONVERSATION HISTORY:`, `RETRIEVED KNOWLEDGE CONTEXT:`, and any `OBSERVATION:` sections when formulating your responses.
    *   If the provided context and tool results are insufficient to answer the user's query, clearly state that you don't have enough information.
    *   Do not invent facts or information not present in the provided context or obtained through your tools.
    *   When you use information recalled via `knowledge_search` or facts from `fact_extractor`, try to phrase your response confidently, e.g., "I recall that..." or "Based on what I know...".

    You will receive input from the system structured like this:
    CONVERSATION HISTORY:
    ... (user and assistant turns) ...

    RETRIEVED KNOWLEDGE CONTEXT:
    ... (information retrieved by knowledge_search if it was run by the system before your current turn) ...

    USER QUERY:
    ... (the user's latest message) ...

    [Or, if a tool was just used by you:]
    OBSERVATION:
    ... (result from your previous tool call) ...

    USER QUERY:
    ... (the original user's query that led to the tool use) ...

    Now, respond to the USER QUERY, using a tool if necessary, or by providing a direct answer.
  # --- End ReAct System Prompt Configuration ---



# Agent specific model overrides
agents:
  # If these agents should use a different (perhaps smaller/faster) model than the CO's main chat model
  summarization_agent_model: "gemma3:4b-it-fp16" # Good choice for a faster summarizer. Ensure pulled.
  fact_extraction_agent_model: "gemma3:4b-it-fp16" # Good choice for faster fact extraction. Ensure pulled.

# Data directory (base for relative paths above)
data_dir: "data"